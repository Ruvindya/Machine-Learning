{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOuBz1Ziq6jTae4W6eOjhRL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ruvindya/Machine-Learning/blob/main/Lab03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LAB 03 2019E117"
      ],
      "metadata": {
        "id": "5yoAalDDsMsA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Study the dataset ’Fashion-MNIST’ in Keras. Answer the following questions in relation to the above dataset."
      ],
      "metadata": {
        "id": "yZbJJDmFsUQc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) Find out whether it can be used for regression or classification."
      ],
      "metadata": {
        "id": "qeWaKZNcswxf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   It is used fot classification."
      ],
      "metadata": {
        "id": "06f1jP7JtHXK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) What is the size of the images?"
      ],
      "metadata": {
        "id": "xTjzs4EYtYyl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   image is a 28 x 28 size grayscale image"
      ],
      "metadata": {
        "id": "W9T6-EeS4LYK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(c) How many images are there in the train data?"
      ],
      "metadata": {
        "id": "qGFqvtrkuRb4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   The fashion MNIST dataset consists of 60,000 images for the training set data\n"
      ],
      "metadata": {
        "id": "eNe0t7zJ3xwa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(d) State the number of images in test data."
      ],
      "metadata": {
        "id": "WvBACQoUu4fB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  10,000 images for the testing set."
      ],
      "metadata": {
        "id": "0QBApgRd5DRC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(e) How many classes are there in the data? Write down those classes."
      ],
      "metadata": {
        "id": "gJb1jSBIvVN7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* T-shirt/top\n",
        "* Trouser\n",
        "* Pullover\n",
        "* Dress\n",
        "* Coat\n",
        "* Sandal\n",
        "* Shirt\n",
        "* Sneaker\n",
        "* Bag\n",
        "* Ankle boot"
      ],
      "metadata": {
        "id": "nc9pe6vl6K5p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Load that dataset directly from Keras using Python."
      ],
      "metadata": {
        "id": "57g5AH7Gwh__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
      ],
      "metadata": {
        "id": "3k_ncWzRvkzT"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. View some images in training data, for example draw the 11th image in your training data."
      ],
      "metadata": {
        "id": "AWxX0sqCyp3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(1,1))\n",
        "plt.imshow(train_images[4000], cmap='gray')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114
        },
        "id": "6YTxuhn4ySOk",
        "outputId": "f27d67b8-e7ab-4907-e60e-8f82da6f60b8"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 100x100 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4. Normalize your data (train and test) between 0 and 1.\n",
        "Hint: This is a grayscale image has pixel values between 0 and 255."
      ],
      "metadata": {
        "id": "wvJTzCSbziZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "# zi = (xi – min(x)) / (max(x) – min(x))\n",
        "# train_images = (train_images - 0)/(255 - 0)\n",
        "# train_images = train_images/ 255\n",
        "\n",
        "# This improves the performance and training stability of the model.\n",
        "# This is required only when features have different ranges."
      ],
      "metadata": {
        "id": "DHkLwW9ky3Hj"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5. Now divide the training data into two: \n",
        "Validation images (first 5000 images from the initial training data) and Training images (rest of the images in your initial training data)."
      ],
      "metadata": {
        "id": "Rr6JoolTzvSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_images = train_images[:5000]\n",
        "val_labels = train_labels[:5000]\n",
        "train_images = train_images[5000:]\n",
        "train_labels = train_labels[5000:]\n",
        "\n",
        "# data splitting is typically done to avoid overfitting\n"
      ],
      "metadata": {
        "id": "ewFOxHIUztsq"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6. Initialize the weight and bias parameters of your model.\n"
      ],
      "metadata": {
        "id": "2J2aDc6q0K0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weight_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.01, seed=None)\n",
        "bias_init = keras.initializers.Zeros()\n",
        "\n",
        "# Weight initialization is used to define the initial values for the parameters in neural network models prior to training the models\n",
        "# without bias weights, your model would have very limited movement when looking for a solution"
      ],
      "metadata": {
        "id": "pWyOSq360Hv6"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##7. Now build the neural network model with the following characteristics:\n",
        "(a) One Flatten layer as the input layer.\n",
        "(b) Two dense relu layers as hidden layers.\n",
        "(c) A dense softmax layer as the output layer\n"
      ],
      "metadata": {
        "id": "Lgd1ljHZ0-9O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(256, activation='relu', kernel_initializer=weight_init, bias_initializer=bias_init),\n",
        "    keras.layers.Dense(128, activation='relu', kernel_initializer=weight_init, bias_initializer=bias_init),\n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Flatten is the function that converts the pooled feature map to a single column that is passed to the fully connected layer. Dense adds the fully connected layer to the neural network"
      ],
      "metadata": {
        "id": "RXetFNgb066p"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##8. Answer the following questions.\n",
        "(a) What is the use of Flatten layer?"
      ],
      "metadata": {
        "id": "R_yUmOrfJeWM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* By using flatten layer, can map the input images int 2D layer which is single column. Then it can compatible with dense layer"
      ],
      "metadata": {
        "id": "ME6tXOXWJn98"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) Generally, softmax activation function is used in the output layer of the classification networks. Why?"
      ],
      "metadata": {
        "id": "wLnBEWmLKTLB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The softmax activation function is commonly used in the output layer to normalizes the output probabilities across classes. Then it is suitable for multi-class classification. Those decimal probabilities must add up to 1.0"
      ],
      "metadata": {
        "id": "1GxLo9iuKY7F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##9. Print the summary of the network"
      ],
      "metadata": {
        "id": "a_Ex-oDfMm_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxOO56_y1HxA",
        "outputId": "c6d82428-b9bc-4187-ff2b-e7376bebea40"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_2 (Flatten)         (None, 784)               0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 256)               200960    \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 128)               32896     \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 235,146\n",
            "Trainable params: 235,146\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##10. Now compile the model with the desired loss function, optimizer and metrics."
      ],
      "metadata": {
        "id": "O8kQr0NdMzdZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "25XDGf0gNAN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "OF_6xlar1MUo"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##11. Now train your model on the training data and validate your model."
      ],
      "metadata": {
        "id": "7hG2ib8LNMVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_images, train_labels, epochs=10, validation_data=(val_images, val_labels))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsJn1OJy1TDp",
        "outputId": "cc41af38-fda5-4ac9-e709-cd3f4faf6680"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1719/1719 [==============================] - 12s 7ms/step - loss: 0.5027 - accuracy: 0.8163 - val_loss: 0.4184 - val_accuracy: 0.8398\n",
            "Epoch 2/10\n",
            "1719/1719 [==============================] - 11s 6ms/step - loss: 0.3686 - accuracy: 0.8639 - val_loss: 0.3673 - val_accuracy: 0.8696\n",
            "Epoch 3/10\n",
            "1719/1719 [==============================] - 11s 7ms/step - loss: 0.3327 - accuracy: 0.8766 - val_loss: 0.3302 - val_accuracy: 0.8772\n",
            "Epoch 4/10\n",
            "1719/1719 [==============================] - 11s 7ms/step - loss: 0.3079 - accuracy: 0.8852 - val_loss: 0.3192 - val_accuracy: 0.8836\n",
            "Epoch 5/10\n",
            "1719/1719 [==============================] - 10s 6ms/step - loss: 0.2877 - accuracy: 0.8941 - val_loss: 0.3239 - val_accuracy: 0.8822\n",
            "Epoch 6/10\n",
            "1719/1719 [==============================] - 11s 7ms/step - loss: 0.2737 - accuracy: 0.8974 - val_loss: 0.3016 - val_accuracy: 0.8942\n",
            "Epoch 7/10\n",
            "1719/1719 [==============================] - 11s 7ms/step - loss: 0.2631 - accuracy: 0.9016 - val_loss: 0.2972 - val_accuracy: 0.8916\n",
            "Epoch 8/10\n",
            "1719/1719 [==============================] - 11s 7ms/step - loss: 0.2507 - accuracy: 0.9059 - val_loss: 0.3006 - val_accuracy: 0.8938\n",
            "Epoch 9/10\n",
            "1719/1719 [==============================] - 11s 7ms/step - loss: 0.2388 - accuracy: 0.9088 - val_loss: 0.3052 - val_accuracy: 0.8928\n",
            "Epoch 10/10\n",
            "1719/1719 [==============================] - 11s 7ms/step - loss: 0.2321 - accuracy: 0.9116 - val_loss: 0.3026 - val_accuracy: 0.8944\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##12. Evaluate your model on the test data. What is the accuracy of your model on the test data?"
      ],
      "metadata": {
        "id": "CHWlIC7jNUp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(\"Accuracy on test data:\", test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcYR3n0D1V5w",
        "outputId": "62e78568-93a0-48ff-d08c-e42e1864fce5"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 0.3424 - accuracy: 0.8859\n",
            "Accuracy on test data: 0.8859000205993652\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##13. Now take the first five samples of test data. Print the actual target classes and the predicted target classes of those five samples"
      ],
      "metadata": {
        "id": "BnThBMEINgv_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict the target classes for the first five samples of the test data\n",
        "class_labels = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',    'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "predictions = model.predict(test_images[:5])\n",
        "predicted_classes = [class_labels[prediction.argmax()] for prediction in predictions]\n",
        "actual_classes = [class_labels[label] for label in test_labels[:5]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-nrTr_n16Wd",
        "outputId": "4a88761e-5770-4712-c47a-5b777573d1d7"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 91ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the actual and predicted target classes\n",
        "for i in range(5):\n",
        "    print(\"Actual:\", actual_classes[i], \"Predicted:\", predicted_classes[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNEHC7CE1_mJ",
        "outputId": "0a85ccba-df55-46dc-9426-c591e98fcfbc"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actual: Ankle boot Predicted: Ankle boot\n",
            "Actual: Pullover Predicted: Pullover\n",
            "Actual: Trouser Predicted: Trouser\n",
            "Actual: Trouser Predicted: Trouser\n",
            "Actual: Shirt Predicted: Shirt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G5Fl51cl2EAZ"
      },
      "execution_count": 69,
      "outputs": []
    }
  ]
}